\section{Methodology}

This research investigates the feasibility and limitations of Small and Large Language Models (SLMs/LLMs) for classifying refactorings in open-source projects. Our methodology builds upon prior techniques for validating refactoring engines \cite{soares2011identifying, mongiovi2015scaling}, while adapting them to the emerging context of automated classification using language models. In particular, we focus on the distinction between \textit{pure} and \textit{floss} refactorings, a problem emphasized in empirical studies that highlight the predominance of floss refactorings in practice \cite{agnihotri2022refactoring,soetens2013floss} and the challenges faced by industry in managing refactoring at scale \cite{ivers2022industry,kim2014microsoft}.

Refactorings are essential for maintaining software quality, but distinguishing between pure refactorings (which preserve behavior) and floss refactorings (which may introduce subtle changes) remains a significant challenge. Our dataset already contained classifications from the established Purity tool, providing a reliable baseline against which we could evaluate the performance of LLMs. This study explores whether LLMs can match or surpass the accuracy of traditional tools in this domain.

Our methodology consists of four main components: the definition of research questions, the baseline tool description, the construction of a dataset, and the experimental procedure. This comprehensive approach ensures reproducibility and allows for thorough comparison between traditional and AI-based methods.

\subsection{Research Questions}

To guide our investigation, we formulated the following research questions (RQs) that address both the practical applicability and comparative performance of LLMs in refactoring classification:

\begin{itemize}
    \item \textbf{RQ1:} To what extent can Large Language Models classify refactoring commits (PURE vs. FLOSS) with accuracy comparable to existing Purity classifications?
    \item \textbf{RQ2:} How do different LLMs (Gemma-2:2b, Mistral, DeepSeek-R1:8b) vary in their classification performance, and what factors influence these differences?
\end{itemize}

These questions are designed to balance performance evaluation with practical considerations, aligning academic rigor with the scalability challenges highlighted in industrial studies \cite{ivers2022industry,kim2014microsoft}. RQ1 establishes a direct comparison with existing tools, while RQ2 explores the variability across different model architectures and sizes, providing insights into the trade-offs between computational cost and classification accuracy.

\subsection{Baseline Tool: Purity}

Before delving into our experimental setup, it is crucial to understand the baseline against which we compare LLM performance. The Purity tool \cite{mongiovi2018detecting} is a state-of-the-art refactoring detection and classification system that has been extensively validated in academic and industrial contexts. Our dataset already contained pre-classified labels generated by Purity, which served as the ground-truth for evaluating LLM performance.

Purity operates by analyzing Git diffs and commit messages to classify refactorings into three categories:
\begin{itemize}
    \item \textbf{PURE:} Refactorings that preserve program behavior without introducing functional changes.
    \item \textbf{FLOSS:} Refactorings that may introduce subtle behavioral changes or are incomplete.
    \item \textbf{NONE:} Commits that do not contain refactoring operations.
\end{itemize}

The tool employs a combination of static analysis, pattern matching, and heuristic rules to achieve high accuracy in classification. Its advantages include:
\begin{itemize}
    \item Proven reliability in large-scale empirical studies \cite{mongiovi2015scaling}.
    \item Ability to handle complex refactoring scenarios across multiple programming languages.
    \item Detailed justifications for each classification decision.
\end{itemize}

However, Purity has limitations that motivate our exploration of LLM-based approaches:
\begin{itemize}
    \item Dependency on predefined rules that may not capture novel refactoring patterns.
    \item Potential scalability issues with very large codebases.
    \item Limited adaptability to new programming paradigms or languages.
\end{itemize}

By using existing Purity classifications as our ground-truth baseline, we ensure that our LLM evaluations are anchored in established, peer-reviewed methodology while exploring the potential advantages of AI-driven classification.

\subsection{Data Sources}

Our dataset consists of \textbf{6,281 commits} collected from diverse open-source repositories hosted on GitHub. Repository selection followed systematic criteria to ensure representativeness and relevance:
\begin{itemize}
    \item Projects written primarily in Java, as this is the focus of most refactoring research.
    \item Active repositories with substantial commit histories (at least 500 commits).
    \item Mix of library and application projects to capture different refactoring patterns.
    \item Exclusion of toy projects or educational repositories to maintain industrial relevance.
\end{itemize}

For each commit, we extracted comprehensive metadata using a custom Git handler:
\begin{itemize}
    \item Full Git diff with context lines.
    \item Commit message and author information.
    \item Repository URL and branch details.
    \item Timestamp and parent commit hashes.
\end{itemize}

The baseline classification labels (PURE/FLOSS/NONE) were already available in our dataset, having been previously generated by the Purity tool. This pre-existing ground-truth allowed us to focus our efforts on LLM-based classification while maintaining a reliable reference point for comparison.

The dataset distribution revealed an interesting imbalance: approximately 35\% PURE, 45\% FLOSS, and 20\% NONE classifications, reflecting real-world refactoring patterns where incomplete or behavior-altering changes are more common.

Data organization followed a dual-format approach to support both automated analysis and manual inspection:
\begin{itemize}
    \item \textbf{CSV tables:} Structured format containing commit hashes, repository links, Purity classifications, tool justifications, and metadata. This format enables efficient statistical analysis and integration with data science workflows.
    \item \textbf{JSON logs:} Hierarchical format storing complete LLM responses for each commit, including successful classifications and detailed failure information. This allows for fine-grained error analysis and debugging of model behavior.
\end{itemize}

This complementary representation ensures both scalability for large-scale experiments and granularity for detailed case studies, addressing the dual needs of quantitative evaluation and qualitative understanding.

\subsection{Procedure}

Our experimental procedure follows a systematic, reproducible workflow designed to minimize bias and ensure fair comparison between traditional and AI-based approaches:

\begin{enumerate}
    \item \textbf{Dataset Preparation:} The existing dataset with pre-classified commits was used in its entirety for evaluation. Prompt development and testing were conducted iteratively on subsets of the data to optimize performance before full-scale classification.
    
    \item \textbf{LLM Setup and Configuration:} We configured three different models through the Ollama platform:
    \begin{itemize}
        \item Gemma-2:2b (lightweight, 2 billion parameters)
        \item Mistral (medium-sized, optimized for efficiency)
        \item DeepSeek-R1:8b (larger model with enhanced reasoning capabilities)
    \end{itemize}
    Each model was run with consistent parameters: temperature 0.1 for deterministic outputs, maximum token limits of 2048, and timeout settings of 60 seconds per query.
    
    \item \textbf{Prompt Engineering:} A structured prompt template was developed and iteratively refined through testing on sample commits. The prompt included:
    \begin{itemize}
        \item Clear task description with examples.
        \item Explicit output format requirements (JSON schema).
        \item Context from commit diff and message.
        \item Instructions for handling uncertainty.
    \end{itemize}
    
    \item \textbf{Classification Execution:} Each commit was processed through our custom LLMHandler, which:
    \begin{itemize}
        \item Constructed the prompt using commit data.
        \item Sent requests to Ollama and captured responses.
        \item Implemented retry mechanisms for failed requests.
        \item Parsed JSON responses and handled malformed outputs.
        \item Logged all interactions for reproducibility.
    \end{itemize}
    
    \item \textbf{Comparison and Analysis:} LLM classifications were compared against the existing Purity labels using multiple metrics. Discrepancies were manually reviewed to identify patterns in model errors.
    
    \item \textbf{Scalability Assessment:} Performance metrics were collected throughout the process, including:
    \begin{itemize}
        \item Response times per commit.
        \item CPU and GPU utilization.
        \item Memory consumption patterns.
        \item Failure rates and recovery effectiveness.
    \end{itemize}
    
    \item \textbf{Error Analysis:} Failed classifications and misclassifications were systematically analyzed to:
    \begin{itemize}
        \item Identify common failure modes.
        \item Assess the impact of prompt variations.
        \item Evaluate the effectiveness of retry strategies.
        \item Understand model limitations in complex scenarios.
    \end{itemize}
\end{enumerate}

Figure~\ref{fig:methodology-pipeline} illustrates the overall pipeline of our methodology, from commit collection to the evaluation stage, comparing LLM classifications against existing Purity labels.

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=.3cm,
    every node/.style={font=\small, align=center},
    process/.style={rectangle, rounded corners, draw=black, fill=gray!10, text width=4cm, minimum height=1cm},
    data/.style={rectangle, draw=black, fill=white, text width=4cm, minimum height=1cm},
    arrow/.style={->, thick}
]

% Nodes (vertical pipeline)
\node[data] (commits) {Open-source commits on CSV \\ (6300 total)};
\node[process, below=of commits] (git) {Git Handler \\ (diffs + metadata)};
\node[process, below=of git] (purity) {Purity Tool \\ (PURE/FLOSS/NONE)};
\node[process, below=of purity] (ollama) {Ollama + SLMs \\ (Gemma, Mistral, DeepSeek)};
\node[data, below=of ollama] (storage) {CSV + JSON Storage};
\node[process, below=of storage] (eval) {Evaluation \\};

% Arrows
\draw[arrow] (commits) -- (git);
\draw[arrow] (git) -- (purity);
\draw[arrow] (purity) -- (ollama);
\draw[arrow] (ollama) -- (storage);
\draw[arrow] (storage) -- (eval);

\end{tikzpicture}
\caption{Vertical methodology pipeline for commit classification comparing LLMs against existing Purity labels.}
\label{fig:methodology-pipeline}
\end{figure}

\subsection{Evaluation Metrics}

Model performance was evaluated using a comprehensive set of metrics that capture different aspects of classification quality and system reliability:

\begin{itemize}
    \item \textbf{Accuracy:} The proportion of correctly classified commits out of the total. While straightforward, this metric can be misleading in imbalanced datasets.
    
    \item \textbf{Precision and Recall:} For each class (PURE, FLOSS, NONE):
    \begin{itemize}
        \item Precision = True Positives / (True Positives + False Positives)
        \item Recall = True Positives / (True Positives + False Negatives)
    \end{itemize}
    These metrics provide insights into the model's ability to avoid false positives and capture all relevant instances.
    
    \item \textbf{F1-Score:} The harmonic mean of precision and recall (2 × Precision × Recall / (Precision + Recall)), offering a balanced view of model performance.
    
    \item \textbf{Time-to-First-Failure (TTFF):} Adapted from scaling evaluations of refactoring validation tools \cite{mongiovi2015scaling}, this measures the time elapsed before the first classification failure occurs in a batch. Lower TTFF indicates more robust performance under load.
    
    \item \textbf{Resource Consumption:} Comprehensive monitoring of:
    \begin{itemize}
        \item CPU utilization during prompt processing and response generation.
        \item GPU memory and compute usage for model inference.
        \item RAM consumption for data handling and caching.
        \item Network I/O for API calls to Ollama.
    \end{itemize}
    
    \item \textbf{Failure Rate:} The proportion of commits for which the LLM failed to return a valid JSON classification, including timeouts, parsing errors, and incomplete responses.
\end{itemize}

All metrics were calculated separately for each model and compared against the existing Purity labels. Statistical significance tests (paired t-tests) were applied to ensure that observed differences were not due to random variation.

\subsection{Methodological Considerations and Limitations}

While our methodology provides a robust framework for evaluating LLM-based refactoring classification, several limitations should be acknowledged:

\begin{itemize}
    \item \textbf{Dataset Bias:} The focus on Java projects may limit generalizability to other programming languages.
    \item \textbf{Label Quality:} Although the existing Purity classifications are from a well-established tool, they may contain inherent errors or inconsistencies that affect our evaluation.
    \item \textbf{Model Variability:} LLM performance can vary based on prompt formulation, temperature settings, and random seed values.
    \item \textbf{Scalability Constraints:} The study was conducted on a single workstation, potentially underestimating performance in distributed environments.
\end{itemize}

To mitigate these limitations, we employed manual verification of edge cases, sensitivity analysis of key parameters, and comprehensive error logging. Future work could extend this methodology to additional programming languages and larger-scale distributed environments.
