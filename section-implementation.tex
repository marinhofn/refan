\section{Implementation}
\label{sec:implementation}

The REFAN system was implemented as a modular collection of Python components,
centered on the \texttt{LLMHandler} component, responsible for orchestrating
prompt construction, model inference via Ollama, JSON repair,
fallback phases, and result persistence. This section details the implementation of REFAN, a system for automated refactoring analysis using LLMs, covering its architecture, data flow, and robustness mechanisms. In this section, we preserve the
original draft text (project structure, code snippets, and main parameters)
and add graphical artifacts and tables that describe the configuration and
execution flows in detail.

\subsection{Implementation overview}
The system implements a modular architecture with clear separation of responsibilities:
\texttt{git\_handler} (diff extraction), \texttt{\_llm\_handler} (inference and recovery),
\texttt{prompt\_builder} (prompt engineering), and \texttt{storage\_manager} (CSV/JSON).
The file tree and main components are documented in the project guide.%
% (origem dos detalhes e estatísticas: ROADMAP.MD). 
%\vspace{0.25cm}
The system relies on Python libraries such as \texttt{requests} for API calls, \texttt{pandas} for data handling, and \texttt{subprocess} for Git operations.

\subsection{Data Flow and Input Processing}
REFAN processes Git repositories by first cloning or accessing them locally. The \texttt{GitHandler} extracts diffs using Git commands, with preprocessing to handle large diffs (e.g., truncation at 60KB). Example code for diff extraction:

\begin{lstlisting}[language=Python]
def extract_commit_diff(self, repo_path: str, commit_hash: str, previous_hash: str) -> str:
    """Extracts diff between commits using Git."""
    cmd = ["git", "diff", previous_hash, commit_hash]
    result = subprocess.run(cmd, cwd=repo_path, capture_output=True, text=True)
    if result.returncode == 0:
        return result.stdout
    else:
        raise Exception(f"Git diff failed: {result.stderr}")
\end{lstlisting}

% --- Handler configuration table (modular)
\subsection{Handler configuration}
The \texttt{LLMHandler} exposes configurable parameters used during the analysis campaign and that strongly influenced robustness and performance. Table~\ref{tab:handler-config} summarizes key parameters, such as timeouts and model selection, which impact performance.
\input{fig/handler_config_table.tex}

% --- Architecture figure
\subsection{LLM Handler Architecture}
The handler's internal architecture, including prompt, execution, repair, and fallback modules, is represented in Figure~\ref{fig:handler-architecture}. The architecture includes modules for prompt engineering, API interaction, and result validation, ensuring modularity.
\input{fig/handler_architecture.tex}

% --- Pipeline pseudocode (mantivemos e expandimos o pseudocódigo do draft)
\subsection{Classification pipeline}
The classification pipeline follows five main phases: data preparation,
diff extraction, prompt construction and inference, response processing
(with repair), and result aggregation. The high-level pseudocode is in
Figure~\ref{fig:classification-pipeline}.
\input{fig/classification_pipeline.tex}

The pipeline includes unit tests for each module, using frameworks like \texttt{pytest}, to ensure reproducibility.

% --- Core algorithm snippet (do draft)
\subsection{Core processing and response handling}

The main function of the (\texttt{\_process\_llm\_response}) handler implements the multi-stage approach: 

\begin{itemize}
  \item \textbf{(1)} attempted removal by FINAL pattern,
  \item \textbf{(2)} JSON removal with attempted repair
  \item \textbf{(3)} fallback to natural text removal
  \item \textbf{(4)} retries with simplified promp
  \item \textbf{(5)} failure logging. Simplified excerpt from the repair flow:
\end{itemize}

\lstinputlisting[
    caption={JSON repair strategy in handler.},
    label={lst:json-repair},
    style=acmpython
]{fig/json_repair_listing.tex}

\input{fig/recovery_flow.tex}

Example code for prompt building:

\begin{lstlisting}[language=Python]
def build_commit_prompt(commit_data: dict, prompt_template: str) -> str:
    """Builds a prompt from commit data."""
    return prompt_template.format(
        repository=commit_data['repository'],
        diff=commit_data['diff'][:60000],  # Truncate for limits
        commit_message=commit_data['commit_message']
    )
\end{lstlisting}



% --- Model stats (tabela modular)
\subsection{Model performance and classification results}
The system's performance was evaluated across multiple LLMs, providing insights into their effectiveness for refactoring classification. Table~\ref{tab:model-performance-rotated} presents comprehensive statistics from the analysis of 2,728 commits, comparing the performance of three different models against the established Purity tool.

\begin{table}[H]
\centering
\caption{Classification performance comparison across models and Purity tool.}
\label{tab:model-performance-rotated}
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Model} & \textbf{Total Analyzed} & \textbf{PURE} & \textbf{FLOSS} & \textbf{PURE \%} \\
\midrule
Purity & 2,728 & 978 & 1,750 & 35.85\% \\
Mistral & 2,728 & 766 & 1,962 & 28.08\% \\
Gemma-2:2b & 2,728 & 561 & 2,167 & 20.56\% \\
DeepSeek-R1:8b & 2,728 & 201 & 2,527 & 7.37\% \\
\bottomrule
\end{tabular}
\end{table}

These results reveal significant variations in classification behavior across the different approaches. The Purity tool, serving as our baseline, classified 35.85\% of commits as pure refactorings, reflecting its rule-based approach that identifies specific structural patterns. In contrast, the LLMs exhibited progressively more conservative behavior:

\begin{itemize}
    \item \textbf{Mistral} showed the most balanced performance with 28.08\% pure classifications, demonstrating reasonable alignment with traditional methods while offering the flexibility of AI-driven analysis.
    \item \textbf{Gemma-2:2b} was more conservative, classifying only 20.56\% as pure, suggesting a tendency toward caution when dealing with ambiguous refactoring scenarios.
    \item \textbf{DeepSeek-R1:8b} exhibited extreme conservatism with just 7.37\% pure classifications, indicating that larger models may prioritize safety over specificity in classification tasks.
\end{itemize}

The observed trend toward more FLOSS classifications by LLMs compared to Purity suggests that these models may be more sensitive to subtle behavioral changes or may err on the side of caution when uncertainty exists. This conservative bias could be beneficial for quality assurance but might result in over-classification of potentially pure refactorings as floss.

These performance metrics were generated through the complete pipeline execution, including prompt engineering, LLM inference via Ollama, response parsing, and error recovery mechanisms. The results highlight both the potential of LLMs for automated refactoring analysis and the importance of careful model selection based on specific use case requirements. 

% --- Error recovery metrics
\subsection{Error Recovery and Failure Taxonomy}
The system implements a robust error recovery mechanism to handle failures in LLM-based commit analysis, ensuring high reliability despite the inherent unpredictability of large language model responses. The implementation includes multiple recovery strategies and comprehensive failure logging.
\subsubsection{Error Recovery Mechanisms}
The core error recovery is implemented in the \texttt{\_processLLMResponse} method of the \texttt{LLMHandler} class. When an LLM response is received, the system attempts the following recovery strategies in order:

\begin{itemize}
    \item \textbf{JSON Extraction:} First attempts to extract valid JSON from the LLM response using pattern matching and parsing utilities.
    \item \textbf{Raw Text Analysis:} If JSON extraction fails, the system falls back to \texttt{\_extractAnalysisFromRawText}, which analyzes the raw response text for classification indicators and creates a structured result.
    \item \textbf{Retry with Simplified Prompt:} If raw text analysis yields insufficient results, the system invokes \\ \texttt{\_retryAnalysisWithSimplifiedPrompt} to make a second LLM call with a simplified, JSON-only prompt.
    \item \textbf{Fallback Result Creation:} As a final resort,\\ \texttt{\_createFallbackResult} generates a conservative result based on keyword analysis of the raw response.
\end{itemize}


\subsubsection{Failure Logging and Taxonomy: }

All failures are logged to \texttt{json\_failures.json} via the \texttt{saveJsonFailure} method, which captures:

\begin{verbatim}
{
  "timestamp": "2025-08-19T22:20:29.036890",
  "commit_hash": "0051...7bd202...3ecb2513eb",
  "repository": "https://github.com/.../...",
  "commit_message": "Merged revisions...",
  "error": "Unable to extract valid JSON from response",
  "llm_response_complete": "<think>...",
  "llm_response_excerpt": "<think>...",
  "analysis_attempt": "JSON parsing failed",
  "prompt_excerpt": "..."
}
\end{verbatim}

\subsubsection{Failures}
\input{fig/failure_type.tex}

\subsubsection{Code Example: Retry Mechanism}

The retry mechanism is detailed in the following section with a complete example.

\subsubsection{Recovery Effectiveness}
The multi-layered recovery approach demonstrates robust error handling by implementing progressive fallback strategies that maintain system continuity. This design ensures that even when initial LLM responses fail, the system can recover through intelligent retry mechanisms and alternative parsing strategies. For instance, the retry mechanism attempts to re-query the LLM with a simplified prompt when JSON parsing fails, as shown in the following code example:

The retry mechanism is broken down into three main parts:

\begin{lstlisting}[language=Python, numbers=left]
def _retry_analysis_with_simplified_prompt(self, commit_hash: str, previous_hash: str, repository: str, original_prompt: str) -> Optional[dict]:
    """Attempts a new analysis with a simplified, JSON-only prompt when the first fails."""
    try:
        print(dim(f"Executing retry for {commit_hash} with simplified prompt"))
        
        # Build simplified prompt with JSON-only instructions
        simplified_prompt = f"""
CRITICAL: You must respond with ONLY a valid JSON object. No other text before or after.

Analyze this Git diff and classify as either "pure" or "floss" refactoring.

Repository: {repository or 'unknown'}
Previous commit: {previous_hash or 'unknown'}  
Current commit: {commit_hash}

Response format (respond with ONLY this JSON structure):
{{
    "repository": "{repository or 'unknown'}",
    "commit_hash_before": "{previous_hash or 'unknown'}",
    "commit_hash_current": "{commit_hash}",
    "refactoring_type": "pure",
    "justification": "Brief analysis of the changes",
    "technical_evidence": "Specific evidence from the diff",
    "confidence_level": "medium",
    "diff_source": "direct"
}}

{original_prompt[-2000:]}  
"""
\end{lstlisting}

\textbf{a) Build simplified prompt:} This section constructs a simplified prompt that enforces JSON-only responses, including the original prompt excerpt.

\begin{lstlisting}[language=Python, numbers=left, firstnumber=25]
        # Make new LLM call with simplified prompt
        response = self._call_ollama(simplified_prompt, model=self.model, attempts=2)
        
        if response:
            # Try to extract JSON from new response
            json_result = extract_json_from_text(response)
            if json_result:
                print(success(f"Retry successful for hash {commit_hash}"))
                return json_result
            else:
                print(warning(f"Retry also failed to extract JSON for hash {commit_hash}"))
        else:
            print(warning(f"Retry did not get LLM response for hash {commit_hash}"))
\end{lstlisting}

\textbf{b) Make new LLM call:} This part executes the LLM call and attempts to extract valid JSON from the response, with success/failure logging.

\begin{lstlisting}[language=Python, numbers=left, firstnumber=40]
    except Exception as e:
        # Handle exceptions and log errors
        print(error(f"Error during retry for hash {commit_hash}: {e}"))
        
    return None
\end{lstlisting}

\textbf{c) Handle exceptions:} Exception handling ensures robust error management during the retry process.

This approach not only recovers from parsing failures but also adapts to different LLM response patterns, making the system resilient to model variations and unexpected outputs.

% \input{fig/error_recovery_table.tex}

\subsection{Data integrity, logging and reproducibility}
The pipeline performs atomic updates of the CSVs, maintains backups and session logs, and produces an aggregated file (\texttt{llm\_analysis\_\allowbreak aggregated.csv}) used to generate the article's tables and figures. The environment and main parameters (models, timeouts, 60KB diff limit) are documented in the configuration repository.

Data integrity is ensured through comprehensive logging mechanisms that capture all failures and recovery attempts. The \\ \texttt{save\_json\_failure} method logs detailed error information to \texttt{json\_failures.json}, including timestamps, commit details, and raw LLM responses:

The failure logging method is broken down into three main parts:

\begin{lstlisting}[language=Python, numbers=left]
def save_json_failure(self, commit_hash: str, repository: str, commit_message: str, raw_response: str, error_msg: str, prompt_excerpt: str | None = None):
    """
    Saves JSON parsing failures to a separate file.
    
    Args:
        commit_hash (str): Hash of the failed commit
        repository (str): Repository of the commit
        commit_message (str): Commit message
        raw_response (str): Complete LLM response
        error_msg (str): Detailed error message
    """
    try:
        # Create failure entry with all relevant details
        failure_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "commit_hash": commit_hash,
            "repository": repository,
            "commit_message": commit_message,
            "error": error_msg,
            "llm_response_complete": raw_response,
            "llm_response_excerpt": raw_response,  # Capture complete response without truncation
            "analysis_attempt": "JSON parsing failed",
            "parse_attempts": 1,
            "llm_prompt_excerpt": prompt_excerpt,
            "notes": "Saved by save_json_failure"
        }
\end{lstlisting}

\textbf{a) Create failure entry:} This section creates a comprehensive failure entry dictionary containing all relevant information about the parsing failure.

\begin{lstlisting}[language=Python, numbers=left, firstnumber=20]
        # Load existing failures from file
        existing_failures = []
        if os.path.exists(self.failures_file):
            try:
                with open(self.failures_file, 'r', encoding='utf-8') as f:
                    existing_failures = json.load(f)
            except json.JSONDecodeError:
                print(warning(f"Failures file {self.failures_file} corrupted, creating new"))
                existing_failures = []
        
        # Add new failure
        existing_failures.append(failure_entry)
\end{lstlisting}

\textbf{b) Load existing failures:} This part safely loads existing failures from the JSON file, handling potential corruption by creating a new list if needed.

\begin{lstlisting}[language=Python, numbers=left, firstnumber=30]
        # Save updated failures list to file
        with open(self.failures_file, 'w', encoding='utf-8') as f:
            json.dump(existing_failures, f, indent=2, ensure_ascii=False)
        
        print(warning(f"JSON failure saved to {self.failures_file} (total: {len(existing_failures)} failures)"))
        
    except Exception as e:
        print(error(f"Error saving JSON failure: {str(e)}"))
\end{lstlisting}

\textbf{c) Save updated failures:} This section appends the new failure and saves the updated list back to the file, ensuring data persistence.

This logging system enables reproducibility by preserving all analysis attempts, failures, and recovery outcomes, facilitating debugging and system improvement.

\subsection{System Limitations and Scalability}
REFAN is optimized for small-to-medium repositories but may face challenges with very large diffs (>100KB) due to LLM context limits. Scalability tests show linear performance degradation with commit volume, mitigated by parallel processing.

The system's performance is influenced by the underlying hardware and software environment. The implementation was developed and tested on a workstation with:

\begin{itemize}
    \item \textbf{GPU}: RTX 4070 Ti Super (16GB VRAM) for accelerated model inference
    \item \textbf{CPU}: Ryzen 7 5700X for parallel processing of commits
    \item \textbf{Memory}: 32GB 3200Mhz DDR4 for handling large datasets and model loading
    \item \textbf{OS}: Windows 11 + WSL2 and Ubuntu 22.04 LTS
\end{itemize}

These specifications ensure robust performance for LLM-based analysis, with the GPU enabling faster inference times and the ample RAM supporting concurrent processing of multiple commits. However, limitations include dependency on Ollama availability and potential biases in LLM responses for ambiguous diffs.

% --- Summary
\subsection{Implementation summary}
With these modifications, the implementation evolved from a one-shot prototype to a robust production system, with a high recovery rate and modularity that allowed for the rapid inclusion of new models and prompt engineering iterations. Future work could include GPU acceleration for larger models and integration with CI/CD pipelines.

The system was developed using a hybrid Windows 11 + WSL2 Ubuntu setup for cross-platform compatibility. This environment enabled efficient development and testing, leveraging the RTX 4070 Ti Super GPU for accelerated LLM inference and the Ryzen 7 5700X CPU for parallel commit processing. The 32GB RAM configuration supports handling large datasets and concurrent model operations, ensuring scalability for medium-sized repositories while maintaining reproducibility through comprehensive logging and atomic CSV updates. Future work could include GPU acceleration for larger models and integration with CI/CD pipelines.
