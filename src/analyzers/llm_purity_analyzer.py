#!/usr/bin/env python3
"""
M√≥dulo para an√°lise LLM espec√≠fica do arquivo hashes_no_rpt_purity_with_analysis.csv
Preenche a coluna llm_analysis com an√°lises de commits de refatoramento.
"""

import pandas as pd
import json
import os
import datetime
from typing import Optional, List, Dict, Any
from pathlib import Path
import time
import sys

from src.handlers.optimized_llm_handler import OptimizedLLMHandler
from src.handlers.git_handler import GitHandler
from src.handlers.data_handler import DataHandler
from src.utils.colors import *

class ProgressBar:
    """Barra de progresso simples para an√°lise LLM."""
    
    def __init__(self, total: int, width: int = 50, title: str = "Progress"):
        self.total = total
        self.current = 0
        self.width = width
        self.title = title
        self.start_time = time.time()
        
    def update(self, current: int = None):
        """Atualiza a barra de progresso."""
        if current is not None:
            self.current = current
        else:
            self.current += 1
            
        # Calcular porcentagem
        percentage = min(100, (self.current / self.total) * 100) if self.total > 0 else 0
        
        # Calcular tempo estimado
        elapsed_time = time.time() - self.start_time
        if self.current > 0:
            avg_time_per_item = elapsed_time / self.current
            remaining_items = self.total - self.current
            eta_seconds = avg_time_per_item * remaining_items
            eta_str = self._format_time(eta_seconds)
        else:
            eta_str = "calculating..."
        
        # Criar barra visual
        filled = int(self.width * percentage / 100)
        bar = '‚ñà' * filled + '‚ñë' * (self.width - filled)
        
        # Formatar elapsed time
        elapsed_str = self._format_time(elapsed_time)
        
        # Imprimir barra
        sys.stdout.write(f'\r{self.title}: |{bar}| {percentage:.1f}% ({self.current}/{self.total}) '
                        f'Elapsed: {elapsed_str} ETA: {eta_str}')
        sys.stdout.flush()
        
        if self.current >= self.total:
            print()  # Nova linha ao completar
            
    def _format_time(self, seconds: float) -> str:
        """Formata tempo em formato leg√≠vel."""
        if seconds < 60:
            return f"{seconds:.0f}s"
        elif seconds < 3600:
            return f"{seconds//60:.0f}m {seconds%60:.0f}s"
        else:
            hours = seconds // 3600
            minutes = (seconds % 3600) // 60
            return f"{hours:.0f}h {minutes:.0f}m"

class LLMPurityAnalyzer:
    """Analisador LLM espec√≠fico para preenchimento da coluna de an√°lise de pureza."""
    
    def __init__(self, model: str | None = None, csv_file_path: str | None = None, dry_run: bool = False):
        """Inicializa o analisador LLM.

        Args:
            model: Nome do modelo LLM a usar (se None usa o atual configurado).
            csv_file_path: Caminho para o CSV a ser usado/atualizado. Se None usa o CSV global.
        """
        self.llm_handler = OptimizedLLMHandler(model=model)
        self.git_handler = GitHandler()
        self.data_handler = DataHandler()
        self.dry_run = dry_run
        # Arquivos de trabalho (dependem do modelo escolhido)
        from src.core.config import get_model_paths, get_current_llm_model, ensure_model_directories
        current_model = model or get_current_llm_model()
        paths = get_model_paths(current_model)
        ensure_model_directories()
        # CSV a utilizar (se informado, usamos esse caminho; caso contr√°rio, o CSV global)
        # Note: default master CSV is the FLOSS purity file
        self.csv_file_path = csv_file_path or "csv/floss_hashes_no_rpt_purity_with_analysis.csv"
        self.backup_dir = str(paths['ANALISES_DIR'])  # Diret√≥rio espec√≠fico do modelo
        self.session_log_file = None

        # Estat√≠sticas da sess√£o
        self.stats = {
            "start_time": datetime.datetime.now(),
            "total_processed": 0,
            "successful_analyses": 0,
            "failed_analyses": 0,
            "skipped_already_analyzed": 0,
            "processing_errors": 0
        }

        os.makedirs(self.backup_dir, exist_ok=True)
        # Flag para evitar cria√ß√£o de m√∫ltiplos backups durante a mesma sess√£o
        self._backup_created = False
        
    def _create_session_log_file(self) -> str:
        """Cria arquivo de log da sess√£o."""
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        log_filename = f"llm_purity_analysis_{timestamp}.json"
        log_path = os.path.join(self.backup_dir, log_filename)
        return log_path
    
    def _load_csv_data(self) -> Optional[pd.DataFrame]:
        """Carrega os dados do CSV de an√°lise de pureza."""
        try:
            if not os.path.exists(self.csv_file_path):
                print(error(f"Arquivo {self.csv_file_path} n√£o encontrado."))
                return None
            
            df = pd.read_csv(self.csv_file_path)
            # Garantir que a coluna llm_analysis exista e seja string para evitar warnings
            if 'llm_analysis' not in df.columns:
                # Criar coluna com dtype string para evitar futuros warnings ao atribuir
                df['llm_analysis'] = pd.Series([''] * len(df), dtype='string')
            else:
                # For√ßar dtype string (preserva valores existentes)
                try:
                    df['llm_analysis'] = df['llm_analysis'].astype('string')
                except Exception:
                    # Em casos estranhos, recriar a coluna como string
                    df['llm_analysis'] = pd.Series(df['llm_analysis'].astype(str).fillna(''), dtype='string')
            print(success(f"Carregados {len(df)} hashes do arquivo de an√°lise."))
            return df
            
        except Exception as e:
            print(error(f"Erro ao carregar CSV: {str(e)}"))
            return None
    
    def _save_csv_data(self, df: pd.DataFrame) -> bool:
        """Salva os dados atualizados no CSV."""
        try:
            # Criar um backup apenas uma vez por sess√£o para evitar polui√ß√£o
            # do diret√≥rio csv. O backup ser√° armazenado no diret√≥rio do modelo
            # (self.backup_dir) para manter os arquivos de trabalho organizados.
            backup_timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
            if (not self._backup_created) and os.path.exists(self.csv_file_path):
                # Nome seguro para o backup
                original_name = Path(self.csv_file_path).name
                backup_path = os.path.join(self.backup_dir, f"{original_name}.backup_{backup_timestamp}")
                df_original = pd.read_csv(self.csv_file_path)
                df_original.to_csv(backup_path, index=False)
                print(info(f"Backup criado: {backup_path}"))
                self._backup_created = True

            # Salvar arquivo atualizado (sobrescreve o CSV de trabalho)
            df.to_csv(self.csv_file_path, index=False)
            print(success(f"Arquivo {self.csv_file_path} atualizado com sucesso."))
            return True
            
        except Exception as e:
            print(error(f"Erro ao salvar CSV: {str(e)}"))
            return False
    
    def _get_commit_data_from_refactoring_csv(self, hash_commit: str) -> Optional[Dict]:
        """Busca dados do commit no arquivo commits_with_refactoring.csv."""
        try:
            # Carregar dados se ainda n√£o foram carregados
            if not self.data_handler.load_data():
                return None
            
            # Buscar o commit pelo hash
            commit_data = self.data_handler.data[
                self.data_handler.data['commit2'] == hash_commit
            ]
            
            if commit_data.empty:
                print(warning(f"Commit {hash_commit[:8]}... n√£o encontrado no arquivo de refatora√ß√µes."))
                return None
            
            # Pegar a primeira ocorr√™ncia se houver duplicatas
            row = commit_data.iloc[0]
            
            return {
                'commit1': row['commit1'],
                'commit2': row['commit2'],
                'project': row['project'],
                'project_name': row['project_name']
            }
            
        except Exception as e:
            print(error(f"Erro ao buscar dados do commit {hash_commit[:8]}...: {str(e)}"))
            return None
    
    def _get_diff_for_commit(self, repository: str, commit1: str, commit2: str) -> Optional[tuple[str, str]]:
        """Obt√©m o diff entre dois commits e retorna tamb√©m o caminho local do reposit√≥rio.

        Returns:
            tuple(diff_content, repo_path) ou None em caso de erro.
        """
        try:
            success, repo_path = self.git_handler.ensure_repo_cloned(repository)
            if not success:
                print(error(f"Falha ao preparar reposit√≥rio: {repository}"))
                return None
            diff_content = self.git_handler.get_commit_diff(repo_path, commit1, commit2)
            if not diff_content:
                print(warning(f"Diff vazio entre commits {commit1[:8]}...{commit2[:8]}"))
                return None
            return diff_content, repo_path
        except Exception as e:
            print(error(f"Erro ao obter diff: {str(e)}"))
            return None
    
    def _analyze_single_commit(self, hash_commit: str, purity_classification: str) -> Optional[Dict]:
        """Analisa um √∫nico commit com a LLM."""
        try:
            print(info(f"Analisando commit {hash_commit[:8]}... (Purity: {purity_classification})"))
            
            # Buscar dados do commit
            commit_data = self._get_commit_data_from_refactoring_csv(hash_commit)
            if not commit_data:
                return None
            
            # Dry-run: n√£o realiza chamadas git/LLM, retorna resultado simulado
            if self.dry_run:
                print(dim(f"Dry-run ativado: simulando an√°lise para {hash_commit[:8]}..."))
                result = {
                    'hash': hash_commit,
                    'purity_classification': purity_classification,
                    'llm_classification': 'DRY_RUN',
                    'llm_justification': 'Dry run - an√°lise simulada (nenhuma chamada LLM foi realizada).',
                    'llm_confidence': '0',
                    'project_name': commit_data.get('project_name', 'unknown'),
                    'analysis_timestamp': datetime.datetime.now().isoformat(),
                    'diff_size': 0,
                    'diff_lines': 0
                }
                print(success(f"‚úÖ Commit {hash_commit[:8]}... simuladamente analisado: {result['llm_classification']}"))
                return result
            
            # Obter diff (com repo_path para evitar novo fetch)
            diff_result = self._get_diff_for_commit(
                commit_data['project'],
                commit_data['commit1'],
                commit_data['commit2']
            )
            if not diff_result:
                return None
            diff_content, repo_path = diff_result

            # Obter mensagem do commit (uma √∫nica vez, usando repo j√° atualizado)
            try:
                commit_message = self.git_handler.get_commit_message(repo_path, commit_data['commit2']) or "Commit message not available"
            except Exception:
                commit_message = "Commit message not available"
            
            # An√°lise com LLM
            try:
                llm_result = self.llm_handler.analyze_commit_refactoring(
                    current_hash=hash_commit,
                    previous_hash=commit_data['commit1'],
                    repository=commit_data['project'],
                    diff_content=diff_content,
                    commit_message=commit_message,
                    repo_path=repo_path
                )
                
                if llm_result and llm_result.get('success') and llm_result.get('refactoring_type'):
                    result = {
                        'hash': hash_commit,
                        'purity_classification': purity_classification,
                        'llm_classification': llm_result['refactoring_type'].upper(),
                        'llm_justification': llm_result.get('justification', ''),
                        'llm_confidence': llm_result.get('confidence_level', 'unknown'),
                        'project_name': commit_data['project_name'],
                        'analysis_timestamp': datetime.datetime.now().isoformat(),
                        'diff_size': len(diff_content),
                        'diff_lines': len(diff_content.splitlines())
                    }
                    
                    print(success(f"‚úÖ Commit {hash_commit[:8]}... analisado: {result['llm_classification']}"))
                    return result
                else:
                    print(error(f"‚ùå Falha na an√°lise LLM do commit {hash_commit[:8]}... - Resultado inv√°lido"))
                    return None
                    
            except Exception as llm_error:
                print(error(f"‚ùå Erro na chamada LLM para {hash_commit[:8]}...: {str(llm_error)}"))
                return None
                
        except Exception as e:
            print(error(f"Erro na an√°lise do commit {hash_commit[:8]}...: {str(e)}"))
            return None
    
    def _save_session_analysis(self, analyses: List[Dict]) -> None:
        """Salva an√°lises da sess√£o em arquivo JSON."""
        try:
            if not self.session_log_file:
                self.session_log_file = self._create_session_log_file()
            
            session_data = {
                "session_info": {
                    "start_time": self.stats["start_time"].isoformat(),
                    "end_time": datetime.datetime.now().isoformat(),
                    "total_processed": self.stats["total_processed"],
                    "successful_analyses": self.stats["successful_analyses"],
                    "failed_analyses": self.stats["failed_analyses"],
                    "skipped_already_analyzed": self.stats["skipped_already_analyzed"],
                    "processing_errors": self.stats["processing_errors"]
                },
                "analyses": analyses
            }
            
            with open(self.session_log_file, 'w', encoding='utf-8') as f:
                json.dump(session_data, f, indent=2, ensure_ascii=False)
            
            print(info(f"Sess√£o salva em: {self.session_log_file}"))
            
        except Exception as e:
            print(error(f"Erro ao salvar sess√£o: {str(e)}"))
    
    def analyze_commits(self, 
                       max_commits: Optional[int] = None, 
                       skip_analyzed: bool = True,
                       purity_filter: Optional[str] = None) -> Dict[str, Any]:
        """
        Analisa commits e preenche a coluna llm_analysis.
        
        Args:
            max_commits: N√∫mero m√°ximo de commits para analisar (None = todos)
            skip_analyzed: Se True, pula commits que j√° t√™m an√°lise LLM
            purity_filter: Filtro por classifica√ß√£o Purity ('TRUE', 'FALSE', 'NONE', None = todos)
            
        Returns:
            Dict com estat√≠sticas da an√°lise
        """
        print(header(f"\n{'='*60}"))
        print(header("INICIANDO AN√ÅLISE LLM DE COMMITS DE REFATORAMENTO"))
        print(header(f"{'='*60}"))
        
        # Carregar dados do CSV
        df = self._load_csv_data()
        if df is None:
            return self.stats
        
        # Aplicar filtros
        analysis_df = df.copy()
        
        # Filtrar por classifica√ß√£o Purity se especificado
        if purity_filter:
            initial_count = len(analysis_df)
            analysis_df = analysis_df[analysis_df['purity_analysis'] == purity_filter]
            print(info(f"Filtro Purity '{purity_filter}': {len(analysis_df)} de {initial_count} commits"))
        
        # Pular commits j√° analisados se solicitado
        if skip_analyzed:
            initial_count = len(analysis_df)
            analysis_df = analysis_df[
                (analysis_df['llm_analysis'].isna()) | 
                (analysis_df['llm_analysis'] == '') |
                (analysis_df['llm_analysis'] == 'None')
            ]
            skipped = initial_count - len(analysis_df)
            self.stats["skipped_already_analyzed"] = skipped
            print(info(f"Pulando {skipped} commits j√° analisados. Restam {len(analysis_df)} para an√°lise."))
        
        # Limitar n√∫mero de commits se especificado
        if max_commits and max_commits > 0:
            analysis_df = analysis_df.head(max_commits)
            print(info(f"Limitando an√°lise a {len(analysis_df)} commits."))
        
        if len(analysis_df) == 0:
            print(warning("Nenhum commit para analisar ap√≥s aplica√ß√£o dos filtros."))
            return self.stats
        
        print(info(f"Iniciando an√°lise de {len(analysis_df)} commits..."))
        
        # Inicializar barra de progresso
        progress_bar = ProgressBar(len(analysis_df), title="LLM Analysis")
        
        # Processar commits
        analyses_results = []
        processed_count = 0
        
        try:
            for idx, row in analysis_df.iterrows():
                processed_count += 1
                self.stats["total_processed"] += 1

                hash_commit = row['hash']
                purity_classification = row['purity_analysis']

                # Atualizar barra de progresso
                progress_bar.update(processed_count)

                # Imprimir detalhes do commit atual (em nova linha ap√≥s a barra)
                print(f"{info(f'Processing:')} {hash_commit[:8]}... (Purity: {purity_classification})")

                try:
                    # Analisar commit
                    result = self._analyze_single_commit(hash_commit, purity_classification)

                    if result:
                        # Atualizar DataFrame
                        classification = result['llm_classification']
                        df.loc[df['hash'] == hash_commit, 'llm_analysis'] = classification

                        analyses_results.append(result)
                        self.stats["successful_analyses"] += 1

                        print(success(f"‚úÖ {hash_commit[:8]}... ‚Üí {classification}"))
                    else:
                        # Marcar como falha
                        df.loc[df['hash'] == hash_commit, 'llm_analysis'] = 'FAILED'
                        self.stats["failed_analyses"] += 1

                        print(error(f"‚ùå Failed: {hash_commit[:8]}..."))

                    # Salvar progresso IMEDIATAMENTE ap√≥s cada commit para permitir
                    # interrup√ß√£o segura (CTRL+C) sem perda de dados.
                    self._save_csv_data(df)
                    self._save_session_analysis(analyses_results)
                    print(dim(f"üíæ Progress saved ({processed_count}/{len(analysis_df)})"))

                    # Pequena pausa entre an√°lises
                    time.sleep(1)

                except Exception as e:
                    self.stats["processing_errors"] += 1
                    df.loc[df['hash'] == hash_commit, 'llm_analysis'] = 'ERROR'
                    print(error(f"‚ö†Ô∏è Error: {hash_commit[:8]}... - {str(e)}"))
                    continue
        except KeyboardInterrupt:
            # Usu√°rio interrompeu com CTRL+C ‚Äî salvar o que foi processado at√© agora
            print(warning('\n‚ö†Ô∏è Interrup√ß√£o detectada (CTRL+C). Salvando progresso atual...'))
            try:
                self._save_csv_data(df)
                self._save_session_analysis(analyses_results)
                print(success('üíæ Progresso salvo com sucesso ap√≥s interrup√ß√£o.'))
            except Exception as e:
                print(error(f"‚ùå Falha ao salvar progresso ap√≥s interrup√ß√£o: {e}"))
            return self.stats

        # Salvar resultados finais
        self._save_csv_data(df)
        self._save_session_analysis(analyses_results)
        
        # Imprimir estat√≠sticas finais
        self._print_final_stats()
        
        return self.stats
    
    def _print_final_stats(self) -> None:
        """Imprime estat√≠sticas finais da an√°lise."""
        end_time = datetime.datetime.now()
        duration = end_time - self.stats["start_time"]
        
        print(f"\n{header('='*60)}")
        print(header("ESTAT√çSTICAS FINAIS DA AN√ÅLISE"))
        print(header(f"{'='*60}"))
        
        print(f"{info('Tempo de execu√ß√£o:')} {duration}")
        print(f"{info('Total processado:')} {self.stats['total_processed']}")
        print(f"{success('An√°lises bem-sucedidas:')} {self.stats['successful_analyses']}")
        print(f"{error('An√°lises falharam:')} {self.stats['failed_analyses']}")
        print(f"{warning('J√° analisados (pulados):')} {self.stats['skipped_already_analyzed']}")
        print(f"{error('Erros de processamento:')} {self.stats['processing_errors']}")
        
        if self.stats['total_processed'] > 0:
            success_rate = (self.stats['successful_analyses'] / self.stats['total_processed']) * 100
            print(f"{info('Taxa de sucesso:')} {success_rate:.1f}%")
        
        if self.session_log_file:
            print(f"{info('Log da sess√£o:')} {self.session_log_file}")
    
    def get_analysis_summary(self) -> Optional[Dict]:
        """Retorna resumo das an√°lises realizadas."""
        try:
            df = self._load_csv_data()
            if df is None:
                return None
            
            # Estat√≠sticas por categoria
            purity_counts = df['purity_analysis'].value_counts()
            llm_counts = df['llm_analysis'].value_counts()
            
            # An√°lise cruzada
            cross_analysis = pd.crosstab(df['purity_analysis'], df['llm_analysis'], margins=True)
            
            summary = {
                "total_hashes": len(df),
                "purity_distribution": purity_counts.to_dict(),
                "llm_distribution": llm_counts.to_dict(),
                "cross_analysis": cross_analysis.to_dict(),
                "completed_analyses": len(df[
                    (df['llm_analysis'].notna()) & 
                    (df['llm_analysis'] != '') & 
                    (~df['llm_analysis'].isin(['FAILED', 'ERROR']))
                ]),
                "pending_analyses": len(df[
                    (df['llm_analysis'].isna()) | 
                    (df['llm_analysis'] == '') |
                    (df['llm_analysis'] == 'None')
                ])
            }
            
            return summary
            
        except Exception as e:
            print(error(f"Erro ao gerar resumo: {str(e)}"))
            return None


def main():
    """Fun√ß√£o principal para execu√ß√£o standalone."""
    analyzer = LLMPurityAnalyzer()
    
    # Exemplo de uso
    print("LLM Purity Analyzer - Op√ß√µes:")
    print("1. Analisar primeiros N commits")
    print("2. Analisar todos os commits")
    print("3. Analisar apenas commits FALSE do Purity")
    print("4. Analisar apenas commits TRUE do Purity")
    print("5. Resumo das an√°lises existentes")
    
    try:
        choice = input("Escolha uma op√ß√£o (1-5): ").strip()
        
        if choice == "1":
            n = int(input("Quantos commits analisar? "))
            analyzer.analyze_commits(max_commits=n)
        elif choice == "2":
            analyzer.analyze_commits()
        elif choice == "3":
            analyzer.analyze_commits(purity_filter="FALSE")
        elif choice == "4":
            analyzer.analyze_commits(purity_filter="TRUE")
        elif choice == "5":
            summary = analyzer.get_analysis_summary()
            if summary:
                print(json.dumps(summary, indent=2))
        else:
            print("Op√ß√£o inv√°lida.")
    
    except KeyboardInterrupt:
        print("\n\nAn√°lise interrompida pelo usu√°rio.")
    except Exception as e:
        print(f"Erro: {str(e)}")


if __name__ == "__main__":
    main()
